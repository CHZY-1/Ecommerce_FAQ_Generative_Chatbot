{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune DialoGPT for E commerce customer service conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face Dataset library\n",
    "\n",
    "https://huggingface.co/docs/datasets/v2.14.4/en/index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/User/.cache/huggingface/datasets/json/default-b1a5f79678292222/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Split =\"train\" to make it Dataset object, by default it load as Dataset Dict object\n",
    "encoded_dataset = load_dataset('json', split='train', data_files=\"Dataset/Encoded_Ecommerce_FAQ_Chatbot_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['questions', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 110\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(encoded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at C:\\Users\\User\\.cache\\huggingface\\datasets\\json\\default-b1a5f79678292222\\0.0.0\\8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\\cache-afdb24240a5283a3.arrow and C:\\Users\\User\\.cache\\huggingface\\datasets\\json\\default-b1a5f79678292222\\0.0.0\\8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\\cache-826fb60ff4597ffb.arrow\n"
     ]
    }
   ],
   "source": [
    "# train, validation test split\n",
    "\n",
    "splited_dataset = encoded_dataset.train_test_split(test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['questions', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 93\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['questions', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 17\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splited_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"Dataset/Encoded_Ecommerce_FAQ_Chatbot_dataset.json\", \"r\", encoding='utf-8') as json_file:\n",
    "#     encoded_dataset = json.load(json_file)\n",
    "\n",
    "# # Convert python list -> numpy array -> pytorch tensor\n",
    "# for key in ['input_ids', 'attention_mask', 'labels']:\n",
    "#     encoded_dataset[key] = torch.tensor(np.array(encoded_dataset[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_dataset['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face Transformer Library documentation\n",
    "\n",
    "https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/trainer#transformers.Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dialoGPT model\n",
    "\n",
    "DialoGPT (from Microsoft Research) released with the paper DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation by Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan.\n",
    "\n",
    "source : https://arxiv.org/abs/1911.00536\n",
    "\n",
    "Microsoft github : https://github.com/microsoft/DialoGPT#retraining-full-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"microsoft/DialoGPT-large\"\n",
    "# model_name = \"microsoft/DialoGPT-medium\"\n",
    "# model_name = \"microsoft/DialoGPT-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum token limit for single sentence: 1024\n"
     ]
    }
   ],
   "source": [
    "max_token_limit = tokenizer.max_len_single_sentence\n",
    "print(f\"Maximum token limit for single sentence: {max_token_limit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face evaluate library\n",
    "\n",
    "https://huggingface.co/docs/evaluate/index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bleu score- https://huggingface.co/spaces/evaluate-metric/bleu\n",
    "\n",
    "evaluate bleu expects :\n",
    "predictions (list of strs): Translations to score.\n",
    "references (list of lists of strs): references for each translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metrics = evaluate.combine(\n",
    "    [\"bleu\", \"meteor\"], force_prefix=True\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = [str(prediction) for prediction in predictions]\n",
    "    references = [[str(reference) for reference in reference_list] for reference_list in labels]\n",
    "\n",
    "    # bleu = evaluate.load(\"bleu\")\n",
    "    # results = bleu.compute(predictions=predictions, references=references)\n",
    "\n",
    "    results = metrics.compute(predictions=predictions, references=references)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./output\",\n",
    "#     overwrite_output_dir=True,\n",
    "#     num_train_epochs=5,\n",
    "#     per_device_train_batch_size=1,\n",
    "#     save_steps=1000,\n",
    "#     save_total_limit=2,\n",
    "#     optim=\"adamw_torch\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/main_classes/trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./logs\", # Output fine-tuned model and logs\n",
    "    overwrite_output_dir=True, # Overwrite the output directory if exists\n",
    "    num_train_epochs=5, # Number of training epochs (110 num rows, allow more epoch for less dataset)\n",
    "    per_device_train_batch_size=2, # Batch size for training (number of samples in each batch)\n",
    "    save_steps=100, # Number of steps before saving a checkpoint\n",
    "    save_total_limit=1, # Maximum number of checkpoints to keep\n",
    "    evaluation_strategy=\"steps\", # Evaluate on validation data at specified steps\n",
    "    eval_steps=100, # Evaluate every 100 steps\n",
    "    logging_steps=25, # Log training information every n steps\n",
    "    learning_rate=1e-4, # start with Low learning rate, dialoGPT paper use 1e-5 to 5e-5.\n",
    "    # warmup_steps=500, # Warm-up steps for learning rate (helps with stability)\n",
    "    weight_decay=0.01, # Weight decay (L2 regularization) parameter (prevent overfitting)\n",
    "    load_best_model_at_end=True, # Load the best model checkpoint at the end of training, only works when evaluation strategy is steps\n",
    "    metric_for_best_model=\"eval_loss\", # Metric to determine best model\n",
    "    optim=\"adamw_torch\" # Optimizer for training - Adamw\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_ = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['questions', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 93\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splited_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['questions', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 17\n",
       "})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_set = splited_dataset['test']\n",
    "\n",
    "validation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStoppingCallback(3, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=splited_dataset['train'],\n",
    "    eval_dataset=splited_dataset['test'],\n",
    "    # tokenizer=tokenizer,\n",
    "    # compute_metrics=compute_metrics,\n",
    "    # data_collator = data_collator_,\n",
    "    callbacks=[early_stop] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids length : 93\n",
      "attention_mask length : 93\n",
      "labels length : 93\n"
     ]
    }
   ],
   "source": [
    "for key in ['input_ids', 'attention_mask', 'labels']:\n",
    "    print(f\"{key} length : {len(splited_dataset['train'][key])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b548339a8804d8987f2dc02f4caea0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.127, 'learning_rate': 8.851063829787234e-05, 'epoch': 0.53}\n",
      "{'loss': 0.6301, 'learning_rate': 7.787234042553192e-05, 'epoch': 1.06}\n",
      "{'loss': 0.3117, 'learning_rate': 6.72340425531915e-05, 'epoch': 1.6}\n",
      "{'loss': 0.2651, 'learning_rate': 5.659574468085107e-05, 'epoch': 2.13}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1cc01f2aa11402f80b1636f815e3f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3466397821903229, 'eval_bleu_bleu': 0.0, 'eval_bleu_precisions': [0.0, 0.0, 0.0, 0.0], 'eval_bleu_brevity_penalty': 1.0, 'eval_bleu_length_ratio': 71.0, 'eval_bleu_translation_length': 1207, 'eval_bleu_reference_length': 17, 'eval_meteor_meteor': 0.0, 'eval_runtime': 71.0908, 'eval_samples_per_second': 0.239, 'eval_steps_per_second': 0.042, 'epoch': 2.13}\n",
      "{'loss': 0.3129, 'learning_rate': 4.595744680851064e-05, 'epoch': 2.66}\n",
      "{'loss': 0.2174, 'learning_rate': 3.531914893617021e-05, 'epoch': 3.19}\n",
      "{'loss': 0.1236, 'learning_rate': 2.468085106382979e-05, 'epoch': 3.72}\n",
      "{'loss': 0.1352, 'learning_rate': 1.4042553191489363e-05, 'epoch': 4.26}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfb18837489e4868a6b487ca670436d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3484913110733032, 'eval_bleu_bleu': 0.0, 'eval_bleu_precisions': [0.0, 0.0, 0.0, 0.0], 'eval_bleu_brevity_penalty': 1.0, 'eval_bleu_length_ratio': 71.0, 'eval_bleu_translation_length': 1207, 'eval_bleu_reference_length': 17, 'eval_meteor_meteor': 0.0, 'eval_runtime': 72.0661, 'eval_samples_per_second': 0.236, 'eval_steps_per_second': 0.042, 'epoch': 4.26}\n",
      "{'loss': 0.0983, 'learning_rate': 3.4042553191489363e-06, 'epoch': 4.79}\n",
      "{'train_runtime': 5657.304, 'train_samples_per_second': 0.082, 'train_steps_per_second': 0.042, 'train_loss': 0.34364249985268774, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=235, training_loss=0.34364249985268774, metrics={'train_runtime': 5657.304, 'train_samples_per_second': 0.082, 'train_steps_per_second': 0.042, 'train_loss': 0.34364249985268774, 'epoch': 5.0})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_bleu_bleu</th>\n",
       "      <th>eval_bleu_precisions</th>\n",
       "      <th>eval_bleu_brevity_penalty</th>\n",
       "      <th>eval_bleu_length_ratio</th>\n",
       "      <th>eval_bleu_translation_length</th>\n",
       "      <th>eval_bleu_reference_length</th>\n",
       "      <th>eval_meteor_meteor</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.1270</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.53</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.6301</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>1.06</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.3117</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>1.60</td>\n",
       "      <td>75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.2651</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>2.13</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.13</td>\n",
       "      <td>100</td>\n",
       "      <td>0.346640</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1207.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.0908</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.042</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.3129</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>2.66</td>\n",
       "      <td>125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.2174</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>3.19</td>\n",
       "      <td>150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.1236</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>3.72</td>\n",
       "      <td>175</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.1352</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>4.26</td>\n",
       "      <td>200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.26</td>\n",
       "      <td>200</td>\n",
       "      <td>0.348491</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1207.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.0661</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.042</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0983</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.79</td>\n",
       "      <td>225</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.00</td>\n",
       "      <td>235</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5657.304</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.042</td>\n",
       "      <td>1.020626e+15</td>\n",
       "      <td>0.343642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      loss  learning_rate  epoch  step  eval_loss  eval_bleu_bleu  \\\n",
       "0   1.1270       0.000089   0.53    25        NaN             NaN   \n",
       "1   0.6301       0.000078   1.06    50        NaN             NaN   \n",
       "2   0.3117       0.000067   1.60    75        NaN             NaN   \n",
       "3   0.2651       0.000057   2.13   100        NaN             NaN   \n",
       "4      NaN            NaN   2.13   100   0.346640             0.0   \n",
       "5   0.3129       0.000046   2.66   125        NaN             NaN   \n",
       "6   0.2174       0.000035   3.19   150        NaN             NaN   \n",
       "7   0.1236       0.000025   3.72   175        NaN             NaN   \n",
       "8   0.1352       0.000014   4.26   200        NaN             NaN   \n",
       "9      NaN            NaN   4.26   200   0.348491             0.0   \n",
       "10  0.0983       0.000003   4.79   225        NaN             NaN   \n",
       "11     NaN            NaN   5.00   235        NaN             NaN   \n",
       "\n",
       "    eval_bleu_precisions  eval_bleu_brevity_penalty  eval_bleu_length_ratio  \\\n",
       "0                    NaN                        NaN                     NaN   \n",
       "1                    NaN                        NaN                     NaN   \n",
       "2                    NaN                        NaN                     NaN   \n",
       "3                    NaN                        NaN                     NaN   \n",
       "4   [0.0, 0.0, 0.0, 0.0]                        1.0                    71.0   \n",
       "5                    NaN                        NaN                     NaN   \n",
       "6                    NaN                        NaN                     NaN   \n",
       "7                    NaN                        NaN                     NaN   \n",
       "8                    NaN                        NaN                     NaN   \n",
       "9   [0.0, 0.0, 0.0, 0.0]                        1.0                    71.0   \n",
       "10                   NaN                        NaN                     NaN   \n",
       "11                   NaN                        NaN                     NaN   \n",
       "\n",
       "    eval_bleu_translation_length  eval_bleu_reference_length  \\\n",
       "0                            NaN                         NaN   \n",
       "1                            NaN                         NaN   \n",
       "2                            NaN                         NaN   \n",
       "3                            NaN                         NaN   \n",
       "4                         1207.0                        17.0   \n",
       "5                            NaN                         NaN   \n",
       "6                            NaN                         NaN   \n",
       "7                            NaN                         NaN   \n",
       "8                            NaN                         NaN   \n",
       "9                         1207.0                        17.0   \n",
       "10                           NaN                         NaN   \n",
       "11                           NaN                         NaN   \n",
       "\n",
       "    eval_meteor_meteor  eval_runtime  eval_samples_per_second  \\\n",
       "0                  NaN           NaN                      NaN   \n",
       "1                  NaN           NaN                      NaN   \n",
       "2                  NaN           NaN                      NaN   \n",
       "3                  NaN           NaN                      NaN   \n",
       "4                  0.0       71.0908                    0.239   \n",
       "5                  NaN           NaN                      NaN   \n",
       "6                  NaN           NaN                      NaN   \n",
       "7                  NaN           NaN                      NaN   \n",
       "8                  NaN           NaN                      NaN   \n",
       "9                  0.0       72.0661                    0.236   \n",
       "10                 NaN           NaN                      NaN   \n",
       "11                 NaN           NaN                      NaN   \n",
       "\n",
       "    eval_steps_per_second  train_runtime  train_samples_per_second  \\\n",
       "0                     NaN            NaN                       NaN   \n",
       "1                     NaN            NaN                       NaN   \n",
       "2                     NaN            NaN                       NaN   \n",
       "3                     NaN            NaN                       NaN   \n",
       "4                   0.042            NaN                       NaN   \n",
       "5                     NaN            NaN                       NaN   \n",
       "6                     NaN            NaN                       NaN   \n",
       "7                     NaN            NaN                       NaN   \n",
       "8                     NaN            NaN                       NaN   \n",
       "9                   0.042            NaN                       NaN   \n",
       "10                    NaN            NaN                       NaN   \n",
       "11                    NaN       5657.304                     0.082   \n",
       "\n",
       "    train_steps_per_second    total_flos  train_loss  \n",
       "0                      NaN           NaN         NaN  \n",
       "1                      NaN           NaN         NaN  \n",
       "2                      NaN           NaN         NaN  \n",
       "3                      NaN           NaN         NaN  \n",
       "4                      NaN           NaN         NaN  \n",
       "5                      NaN           NaN         NaN  \n",
       "6                      NaN           NaN         NaN  \n",
       "7                      NaN           NaN         NaN  \n",
       "8                      NaN           NaN         NaN  \n",
       "9                      NaN           NaN         NaN  \n",
       "10                     NaN           NaN         NaN  \n",
       "11                   0.042  1.020626e+15    0.343642  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(trainer.state.log_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"fine_tuned_dialogpt_FAQ_Ecommerce_large_2\")\n",
    "# tokenizer.save_pretrained(\"tokenizer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatterbot_env_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
