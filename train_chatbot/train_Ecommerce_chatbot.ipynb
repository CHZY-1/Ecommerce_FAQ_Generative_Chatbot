{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face Dataset library\n",
    "\n",
    "https://huggingface.co/docs/datasets/v2.14.4/en/index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/User/.cache/huggingface/datasets/json/default-b1a5f79678292222/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Split =\"train\" to make it Dataset object, by default it load as Dataset Dict object\n",
    "encoded_dataset = load_dataset('json', split='train', data_files=\"Dataset/Encoded_Ecommerce_FAQ_Chatbot_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['questions', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 110\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(encoded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, validation test split\n",
    "\n",
    "splited_dataset = encoded_dataset.train_test_split(test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['questions', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 93\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['questions', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 17\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splited_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"Dataset/Encoded_Ecommerce_FAQ_Chatbot_dataset.json\", \"r\", encoding='utf-8') as json_file:\n",
    "#     encoded_dataset = json.load(json_file)\n",
    "\n",
    "# # Convert python list -> numpy array -> pytorch tensor\n",
    "# for key in ['input_ids', 'attention_mask', 'labels']:\n",
    "#     encoded_dataset[key] = torch.tensor(np.array(encoded_dataset[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_dataset['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face Transformer Library documentation\n",
    "\n",
    "https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/trainer#transformers.Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dialoGPT model\n",
    "\n",
    "DialoGPT (from Microsoft Research) released with the paper DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation by Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan.\n",
    "\n",
    "source : https://arxiv.org/abs/1911.00536\n",
    "\n",
    "Microsoft github : https://github.com/microsoft/DialoGPT#retraining-full-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"microsoft/DialoGPT-large\"\n",
    "# model_name = \"microsoft/DialoGPT-medium\"\n",
    "# model_name = \"microsoft/DialoGPT-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum token limit for single sentence: 1024\n"
     ]
    }
   ],
   "source": [
    "max_token_limit = tokenizer.max_len_single_sentence\n",
    "print(f\"Maximum token limit for single sentence: {max_token_limit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face evaluate library\n",
    "\n",
    "https://huggingface.co/docs/evaluate/index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bleu score- https://huggingface.co/spaces/evaluate-metric/bleu\n",
    "\n",
    "evaluate bleu expects :\n",
    "predictions (list of strs): Translations to score.\n",
    "references (list of lists of strs): references for each translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = [str(prediction) for prediction in predictions]\n",
    "    references = [[str(reference) for reference in reference_list] for reference_list in labels]\n",
    "    \n",
    "    # perplexity, BLEU score, ROUGE score\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    results = bleu.compute(predictions=predictions, references=references)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./output\",\n",
    "#     overwrite_output_dir=True,\n",
    "#     num_train_epochs=5,\n",
    "#     per_device_train_batch_size=1,\n",
    "#     save_steps=1000,\n",
    "#     save_total_limit=2,\n",
    "#     optim=\"adamw_torch\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./logs\", # Output fine-tuned model and logs\n",
    "    overwrite_output_dir=True, # Overwrite the output directory if exists\n",
    "    num_train_epochs=5, # Number of training epochs (110 num rows, allow more epoch for less dataset)\n",
    "    per_device_train_batch_size=2, # Batch size for training (number of samples in each batch)\n",
    "    save_steps=100, # Number of steps before saving a checkpoint\n",
    "    save_total_limit=3, # Maximum number of checkpoints to keep\n",
    "    evaluation_strategy=\"steps\", # Evaluate on validation data at specified steps\n",
    "    eval_steps=100, # Evaluate every 500 steps\n",
    "    logging_steps=25, # Log training information every n steps\n",
    "    learning_rate=1e-4, # start with Low learning rate, dialoGPT paper use 1e-5 to 5e-5.\n",
    "    # warmup_steps=500, # Warm-up steps for learning rate (helps with stability)\n",
    "    # weight_decay=0.01, # Weight decay (L2 regularization) parameter (prevent overfitting)\n",
    "    load_best_model_at_end=True, # Load the best model checkpoint at the end of training, only works when evaluation strategy is steps\n",
    "    metric_for_best_model=\"eval_loss\", # Metric to determine best model\n",
    "    optim=\"adamw_torch\" # Optimizer used for training - Adamw\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_ = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['questions', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 93\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splited_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['questions', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 17\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_set = splited_dataset['test']\n",
    "\n",
    "validation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=splited_dataset['train'],\n",
    "    eval_dataset=splited_dataset['test'],\n",
    "    # tokenizer=tokenizer,\n",
    "    # compute_metrics=compute_metrics,\n",
    "    # data_collator = data_collator_\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids length : 93\n",
      "attention_mask length : 93\n",
      "labels length : 93\n"
     ]
    }
   ],
   "source": [
    "for key in ['input_ids', 'attention_mask', 'labels']:\n",
    "    print(f\"{key} length : {len(splited_dataset['train'][key])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca4a29ed61544592b25ee3b5c8c0837f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6248, 'learning_rate': 1e-05, 'epoch': 1.06}\n",
      "{'loss': 0.8983, 'learning_rate': 2e-05, 'epoch': 2.13}\n",
      "{'loss': 0.4606, 'learning_rate': 3e-05, 'epoch': 3.19}\n",
      "{'loss': 0.3149, 'learning_rate': 4e-05, 'epoch': 4.26}\n",
      "{'train_runtime': 5412.5213, 'train_samples_per_second': 0.086, 'train_steps_per_second': 0.043, 'train_loss': 0.7423461345916099, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=235, training_loss=0.7423461345916099, metrics={'train_runtime': 5412.5213, 'train_samples_per_second': 0.086, 'train_steps_per_second': 0.043, 'train_loss': 0.7423461345916099, 'epoch': 5.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.6248</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.06</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.8983</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>2.13</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.4606</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>3.19</td>\n",
       "      <td>150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.3149</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>4.26</td>\n",
       "      <td>200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.00</td>\n",
       "      <td>235</td>\n",
       "      <td>5412.5213</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.043</td>\n",
       "      <td>1.011922e+15</td>\n",
       "      <td>0.742346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     loss  learning_rate  epoch  step  train_runtime  \\\n",
       "0  1.6248        0.00001   1.06    50            NaN   \n",
       "1  0.8983        0.00002   2.13   100            NaN   \n",
       "2  0.4606        0.00003   3.19   150            NaN   \n",
       "3  0.3149        0.00004   4.26   200            NaN   \n",
       "4     NaN            NaN   5.00   235      5412.5213   \n",
       "\n",
       "   train_samples_per_second  train_steps_per_second    total_flos  train_loss  \n",
       "0                       NaN                     NaN           NaN         NaN  \n",
       "1                       NaN                     NaN           NaN         NaN  \n",
       "2                       NaN                     NaN           NaN         NaN  \n",
       "3                       NaN                     NaN           NaN         NaN  \n",
       "4                     0.086                   0.043  1.011922e+15    0.742346  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(trainer.state.log_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"fine_tuned_dialogpt_FAQ_Ecommerce_1\")\n",
    "# tokenizer.save_pretrained(\"tokenizer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatterbot_env_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
