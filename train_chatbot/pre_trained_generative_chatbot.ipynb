{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Learning Chatbot - Generative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "task : Retrain dialoGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dialoGPT model\n",
    "\n",
    "DialoGPT (from Microsoft Research) released with the paper DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation by Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan.\n",
    "\n",
    "source : https://arxiv.org/abs/1911.00536\n",
    "\n",
    "Microsoft github : https://github.com/microsoft/DialoGPT#retraining-full-models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize pre trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"microsoft/DialoGPT-large\"\n",
    "model_name = \"microsoft/DialoGPT-medium\"\n",
    "# model_name = \"microsoft/DialoGPT-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_intent_data(file_path):\n",
    "    # load intent based json format dataset\n",
    "\n",
    "    with open(file_path, \"r\", encoding='utf-8') as json_file:\n",
    "            data = json.load(json_file)\n",
    "    return data['intents']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_data = load_intent_data('Dataset/intents_it.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_dict({\n",
    "    'dialog': [intent['patterns'] for intent in intent_data],\n",
    "    'response': [intent['responses'][0] for intent in intent_data]  # Using the first response as the target\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(data):\n",
    "\n",
    "    dialog_responses = [\n",
    "        f\"User: {' '.join(dialog)}\\nAssistant: {response}\" \n",
    "        for dialog, response in zip(data['dialog'], data['response'])\n",
    "    ]\n",
    "\n",
    "    encoded_inputs = tokenizer(dialog_responses)\n",
    "    \n",
    "    # encoded_inputs = tokenizer(\n",
    "    #     dialog_responses,\n",
    "    #     padding='max_length',\n",
    "    #     truncation=True,\n",
    "    #     max_length=512,\n",
    "    #     return_tensors='pt'\n",
    "    # )\n",
    "    \n",
    "    return {\n",
    "        'input_ids': encoded_inputs['input_ids'],\n",
    "        'attention_mask': encoded_inputs['attention_mask'],\n",
    "        'labels': encoded_inputs['input_ids']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6159b2b59464df6898e2f12ac40b2e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(encode_data, batched=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['dialog', 'response', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 18\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(encoded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "['Hello, Welcome to Our IT Chatbot', 'See you!', 'Happy to help! Any other issues?', \"Sorry, can't understand you\", 'I can guide you through\\n 1)Password Reset\\n2) Trouble-Shooting issues\\n3) Virus Issues\\n4) Printing Issues\\n5) many more IT issues', 'Confirm your email address', 'The reset pin has been sent on your registered mobile number', 'Simply rebooting can fix the blue screen of death (or STOP error, as it is otherwise known).', 'The first step is to check the recycle bin. If that fails, you can contact your IT Support partner.', 'However, all your work is not definitely lost. If you have Auto-Recover options enabled in Microsoft Office, then there are some easy steps to recover your work.\\nIf not, you can also search for Word backup files by clicking “open”, “computer” and then browsing to the folder where the file was last saved.\\nYou may also be able to find your file by performing a search on your computer for temporary files with a .tmp file extension or a ~ prefix.', 'Here are my 10 quick tips for things you can do to speed up your PC\\n1. Cut down on start-up items\\n2. Uninstall programs you no longer use\\n3. Clean up your hard drive\\n4. Clean your browser\\n5. Scan for and remove malware\\n6. Adjust for better performance\\n7. Defrag your hard drive\\n8. Add more RAM.\\n9. Upgrade to an SSD drive\\n10. Don’t shut down, use Hibernate ', 'This could be related to overheating. Check your computer for dust, and make sure it is in a cool and ventilated place. If this is not the issue, then it is likely a virus problem. Disconnect the PC from any networks and call your IT Support experts!', '\\nIf its not an obvious issue, there may be a connection problem-the printer is probably not connected to the network properly, so call your IT Support helpdesk for help.', 'Hey, I cant seems that you have not listed you Issue here.\\n I can help you out with the following issues: \\n 1)Password Reset\\n 2) Trouble-Shooting issues\\n 3) Virus Issues \\n 4)Printing Issues and many more IT issues', 'A detailed step by step guide to remove the virus from the computer has been provided on the following link: \\nhttps://www.easeus.com/file-recovery/remove-virus-without-antivirus.html \\nand\\nhttps://www.pcworld.com/article/243818/how-to-remove-malware-from-your-windows-pc.html\\nIf the issues are still there, consult the IT team', 'a) If you encounter a mouse problem, you should first try these options:\\n1) If it is a first-time issue, restarting your PC can resolve the issue instantly.\\n2) Confirm that the mouse or the wireless adaptor is firmly connected to the PC.\\n3) You may also try to unplug the mouse cable or the wireless adaptor and reconnect using a different port.\\n4)Check the mouse and the ports for damages and even try the mouse on a different computer.\\n4 If none of these solves the problem, you can now proceed to other solutions.\\nb) Troubleshoot Hardware and Devices\\nc) Updating Incompatible Mouse Drivers\\nd) Roll Back or Reinstall Mouse Drivers\\ne) Deactivate Enhanced Pointer Precision\\nf) Adjusting the Mouse Sensitivity\\ng) Configure Touchpad Delay\\nh) Disable Touchpad', 'First things to check are:\\n1) Does the device work in a different USB port on the machine?\\n2) Are other devices recognised in that port?\\n3) Does the device work on another user’s machine?\\n If you have tried these troubleshooting methods and still no luck, then your IT support help-desk can proceed with some more in-depth troubleshooting.', \"If you're connecting wirelessly, then the location may be the problem. The signal is not necessarily strong in all corners of the building. Similarly, you could just be too far away. If this is not the issue, then spyware or viruses are a likely cause.\"]\n"
     ]
    }
   ],
   "source": [
    "print(len(encoded_dataset['dialog']))\n",
    "\n",
    "print(encoded_dataset['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_and_preprocess_data(file_path):\n",
    "\n",
    "#     with open(file_path, \"r\") as json_file:\n",
    "#         data = json.load(json_file)\n",
    "\n",
    "#     conversations = []\n",
    "#     processed_dialogues = []\n",
    "\n",
    "#     for intent in data['intents']:\n",
    "\n",
    "#         for pattern, response in zip(intent['patterns'], intent['responses']):\n",
    "\n",
    "#             # make it pattern|response pair\n",
    "#             conversation = f\"User: {pattern}|Bot: {response}\"\n",
    "#             conversations.append(conversation)\n",
    "\n",
    "#     print(conversations)\n",
    "\n",
    "#     # Perform Tokenization and preprocess each dialogue in the conversations\n",
    "#     for dialogue in conversations:\n",
    "\n",
    "#         user_input, bot_response = dialogue.split('|')\n",
    "\n",
    "#         # Tokenize user input and bot response separately\n",
    "#         user_input_tokens = tokenizer.encode(user_input.strip(), add_special_tokens=False)\n",
    "#         bot_response_tokens = tokenizer.encode(bot_response.strip(), add_special_tokens=False)\n",
    "\n",
    "#         processed_dialogues.append((user_input_tokens, bot_response_tokens))\n",
    "\n",
    "#     return processed_dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_data = load_and_preprocess_data(file_path=\"Dataset\\intents_1.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face Transformer Library documentation\n",
    "\n",
    "https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/trainer#transformers.Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Data format : input_tokens_n [SEP] [MASK] [target_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "# batching and processing the training data for model\n",
    "\n",
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./output\",   # output directory\n",
    "#     num_train_epochs=10,             # total number of training epochs\n",
    "#     per_device_train_batch_size=16,  # batch size per device during training\n",
    "#     per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "#     warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "#     weight_decay=0.01,               # strength of weight decay\n",
    "#     logging_dir=None,                # directory for storing logs\n",
    "#     fp16=True                        # use floating point 16 bit precision for training\n",
    "#     gradient_accumulation_steps=8,\n",
    "#     logging_steps=100,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training param\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=1,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    optim=\"adamw_torch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['dialog', 'response', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 18\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(encoded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Dataset type: <class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoded Dataset type:\", type(encoded_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs Length: 18\n",
      "Attention Mask Length: 18\n",
      "Labels Length: 18\n"
     ]
    }
   ],
   "source": [
    "input_ids_length = len(encoded_dataset['input_ids'])\n",
    "attention_mask_length = len(encoded_dataset['attention_mask'])\n",
    "labels_length = len(encoded_dataset['labels'])\n",
    "\n",
    "print(\"Input IDs Length:\", input_ids_length)\n",
    "print(\"Attention Mask Length:\", attention_mask_length)\n",
    "print(\"Labels Length:\", labels_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ee3172f4ac445dac60760bf426d585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 212.3662, 'train_samples_per_second': 0.424, 'train_steps_per_second': 0.424, 'train_loss': 2.4217152913411457, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=90, training_loss=2.4217152913411457, metrics={'train_runtime': 212.3662, 'train_samples_per_second': 0.424, 'train_steps_per_second': 0.424, 'train_loss': 2.4217152913411457, 'epoch': 5.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer\\\\tokenizer_config.json',\n",
       " 'tokenizer\\\\special_tokens_map.json',\n",
       " 'tokenizer\\\\vocab.json',\n",
       " 'tokenizer\\\\merges.txt',\n",
       " 'tokenizer\\\\added_tokens.json',\n",
       " 'tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"tuned_dialogpt\")\n",
    "tokenizer.save_pretrained(\"tokenizer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatterbot_env_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
