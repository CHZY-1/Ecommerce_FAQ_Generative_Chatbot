{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting up environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "States: The conversation history or context represented as text sequences. (User question and chatbot respond pair)\n",
    "\n",
    "Actions: Responses that the chatbot can generate in response to user input.\n",
    "\n",
    "Reward: Positive feedback +1 indicate that the chatbot's response was well-received, while negative feedback -1 suggests the response needs improvement. A null value can be treated as neutral or no feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "#list the current work dir\n",
    "os.getcwd()\n",
    "\n",
    "# sys.path is a list of absolute path strings\n",
    "sys.path.append('C:/Users/User/Desktop/Github/Chatbot_Web_Application')\n",
    "sys.path.append(\"D:/GitProjects/Chatbot_Web_Application\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import requests\n",
    "from web_chatbot_app.manage_chat import load_chat_history, save_chat_history\n",
    "from web_chatbot_app.chatbot import Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_HISTORY_PATH = \"C:/Users/User/Desktop/Github/Chatbot_Web_Application/chat_history.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_user_question_space(json_file):\n",
    "    # Load the JSON data from the file\n",
    "    data = load_chat_history(json_file)\n",
    "\n",
    "    # Extract user messages (case insensitive)\n",
    "    user_messages = [entry['user_message'].lower() for entry in data]\n",
    "\n",
    "    # Count unique user questions\n",
    "    unique_user_questions = list(set(user_messages))\n",
    "    user_question_space_size = len(unique_user_questions)\n",
    "\n",
    "    return user_question_space_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'user_message': 'Hello', 'chatbot_response': 'Hello and thank you for the offer to join the team', 'feedback': None}, {'user_message': 'can you help me on my account?', 'chatbot_response': \"I can't do that.please contact our customer support team for assistance.\", 'feedback': None}, {'user_message': 'how do i contact?', 'chatbot_response': 'Contact our customer team for further guidance.', 'feedback': None}, {'user_message': 'hello', 'chatbot_response': 'Hello and thank you for the offer to join the team', 'feedback': None}, {'user_message': 'team?', 'chatbot_response': 'Team is a team of people that are currently in the team and will be joining the team soon', 'feedback': None}, {'user_message': 'why you joining the team?', 'chatbot_response': 'I will be a part of the team as a part time player and as a team we will be working on the team to create a team and future', 'feedback': None}, {'user_message': 'can you tell me the team name?', 'chatbot_response': 'The team is currently in a team with the team.', 'feedback': -1}, {'user_message': 'Hello, assistant', 'chatbot_response': \"Hello, I'm a product manager for a major product manufacturer. Please contact our customer support team with your order details to request a product review or to request an invoice.\", 'feedback': None}, {'user_message': 'how do i request an invoice?', 'chatbot_response': 'You can contact our support team for an invoice or for a product description. We will do our best to assist you with the necessary steps.', 'feedback': None}, {'user_message': 'can you provide me your email address?', 'chatbot_response': 'I will PM you the email address.', 'feedback': None}, {'user_message': 'tell me something about you', 'chatbot_response': 'I have a great personality and a great sense of humor', 'feedback': None}, {'user_message': 'tell me a joke', 'chatbot_response': 'Tell me a story', 'feedback': None}, {'user_message': 'no story available', 'chatbot_response': 'tell a story about a great person', 'feedback': None}, {'user_message': 'hello', 'chatbot_response': 'Hello and thank you for the offer to join the team', 'feedback': -1}, {'user_message': 'thanks for your help', 'chatbot_response': 'You are welcome', 'feedback': 1}, {'user_message': 'hello', 'chatbot_response': 'Hello and thank you for the offer to join the team', 'feedback': None}, {'user_message': 'Hello', 'chatbot_response': 'Hello to you as well', 'feedback': None}, {'user_message': 'can you help me', 'chatbot_response': 'message me your email and I will add you to the team and we can discuss the details', 'feedback': None}, {'user_message': 'hello', 'chatbot_response': 'Hi can I join the chat and discuss the team details', 'feedback': None}]\n"
     ]
    }
   ],
   "source": [
    "data = load_chat_history(CHAT_HISTORY_PATH)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# import nltk\n",
    "\n",
    "# nltk.download('punkt')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotEnv(gym.Env):\n",
    "    def __init__(self, chatbot_api_url, max_episode_length, conversation_json, chatbot):\n",
    "        super(ChatbotEnv, self).__init__()\n",
    "        self.chatbot_api_url = chatbot_api_url\n",
    "        self.max_episode_length = max_episode_length\n",
    "        self.current_step = 0  # Initialize the current step\n",
    "        self.chat_history = load_chat_history(conversation_json)\n",
    "        self.tfidf_vectorizer = TfidfVectorizer()\n",
    "        self.chatbot = chatbot\n",
    "        # print(self.chat_history )\n",
    "\n",
    "        # Define observation space\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'user_question': spaces.Discrete(count_user_question_space(CHAT_HISTORY_PATH)),\n",
    "            'chatbot_response': spaces.Discrete(10),\n",
    "            'feedback': spaces.Discrete(3),  # -1, 0, or 1 for feedback\n",
    "        })\n",
    "\n",
    "        # print(self.observation_space)\n",
    "        \n",
    "        # Define action space\n",
    "        # One actions: 0 -> chatbot generate response\n",
    "        self.action_space = spaces.Discrete(1)  \n",
    "\n",
    "    def step(self, action):\n",
    "        user_question = self.get_user_question()  # Get the next user question from the conversation history\n",
    "        response = self.chatbot.generate_responses(user_question)  # Generate chatbot response\n",
    "\n",
    "        observation = self._encode_observation(user_question, response, self.get_feedback())\n",
    "        reward = self._calculate_reward(response)\n",
    "        self.current_step += 1\n",
    "\n",
    "        done = self.current_step >= self.max_episode_length or self._is_terminal_response(response)\n",
    "\n",
    "        return observation, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.chat_history = []\n",
    "        \n",
    "        # Generate the initial observation when the conversation is reset\n",
    "        initial_observation = self._encode_observation(\n",
    "            user_question=\"Hello\",\n",
    "            chatbot_response=\"Hello\",\n",
    "            feedback=1  # You can set this to None or a default value\n",
    "        )\n",
    "        \n",
    "        return initial_observation\n",
    "\n",
    "    def render(self):\n",
    "        # Implement a render function if you want to visualize or log the environment\n",
    "        pass\n",
    "\n",
    "    def _encode_observation(self, user_question, chatbot_response, feedback):\n",
    "        user_question_embedding = self.encode_text(user_question)\n",
    "        chatbot_response_embedding = self.encode_text(chatbot_response)\n",
    "\n",
    "        observation = {\n",
    "            'user_question': user_question_embedding,\n",
    "            'chatbot_response': chatbot_response_embedding,\n",
    "            'feedback': feedback,\n",
    "        }\n",
    "\n",
    "        return observation\n",
    "    \n",
    "    def encode_text(self, text):\n",
    "        # Fit and transform the text using the updated TF-IDF vectorizer\n",
    "        tfidf_vector = self.tfidf_vectorizer.fit_transform([text])\n",
    "\n",
    "        # Convert the TF-IDF vector to a dense numpy array\n",
    "        encoded_text = tfidf_vector.toarray()[0]\n",
    "\n",
    "        return encoded_text\n",
    "\n",
    "    def _calculate_reward(self, response):\n",
    "        # Get Reward based on chatbot response\n",
    "        # in this case 0, 1, -1\n",
    "        feedback = self.get_feedback()\n",
    "        if feedback is not None:\n",
    "            return feedback  # Use feedback as the reward\n",
    "        else:\n",
    "            return 0  # null in feedback, neutral\n",
    "\n",
    "    def _is_terminal_response(self, response):\n",
    "        # Check if chatbot response indicates the end of an episode\n",
    "        return \"Goodbye\" in response  # Episode terminates if chatbot says \"Goodbye\"\n",
    "\n",
    "    def get_user_question(self):\n",
    "        # Get the user's question from the chat history\n",
    "        if self.current_step < len(self.chat_history):\n",
    "            return self.chat_history[self.current_step][\"user_message\"]\n",
    "        else:\n",
    "            return \"User question not available\"\n",
    "\n",
    "    def get_feedback(self):\n",
    "        # Get feedback for the current chatbot response from the chat history\n",
    "        if self.current_step < len(self.chat_history):\n",
    "            return self.chat_history[self.current_step][\"feedback\"]\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model : C:/Users/User/Desktop/tuned_dialogpt_Ecommerce_FAQ ...\n",
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "chatbot = Chatbot(model=\"C:/Users/User/Desktop/tuned_dialogpt_Ecommerce_FAQ\", \n",
    "                      tokenizer=\"microsoft/DialoGPT-large\")\n",
    "\n",
    "env = ChatbotEnv('http://127.0.0.1:5000/chat', 30, CHAT_HISTORY_PATH, chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level='ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated response length: 71\n",
      "Observation: {'user_question': array([0.5, 0.5, 0.5, 0.5]), 'chatbot_response': array([0.2773501, 0.2773501, 0.2773501, 0.2773501, 0.2773501, 0.2773501,\n",
      "       0.2773501, 0.2773501, 0.5547002, 0.2773501]), 'feedback': None}\n",
      "Reward: 0\n",
      "Done: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated response length: 44\n",
      "Observation: {'user_question': array([0.5, 0.5, 0.5, 0.5]), 'chatbot_response': array([0.35355339, 0.35355339, 0.35355339, 0.35355339, 0.35355339,\n",
      "       0.35355339, 0.35355339, 0.35355339]), 'feedback': None}\n",
      "Reward: 0\n",
      "Done: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated response length: 76\n",
      "Observation: {'user_question': array([0.5, 0.5, 0.5, 0.5]), 'chatbot_response': array([0.30151134, 0.30151134, 0.30151134, 0.30151134, 0.30151134,\n",
      "       0.30151134, 0.30151134, 0.30151134, 0.30151134, 0.30151134,\n",
      "       0.30151134]), 'feedback': None}\n",
      "Reward: 0\n",
      "Done: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated response length: 87\n",
      "Observation: {'user_question': array([0.5, 0.5, 0.5, 0.5]), 'chatbot_response': array([0.24253563, 0.48507125, 0.24253563, 0.24253563, 0.24253563,\n",
      "       0.24253563, 0.48507125, 0.24253563, 0.24253563, 0.24253563,\n",
      "       0.24253563]), 'feedback': None}\n",
      "Reward: 0\n",
      "Done: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated response length: 66\n",
      "Observation: {'user_question': array([0.5, 0.5, 0.5, 0.5]), 'chatbot_response': array([0.28867513, 0.28867513, 0.28867513, 0.28867513, 0.28867513,\n",
      "       0.28867513, 0.28867513, 0.28867513, 0.28867513, 0.28867513,\n",
      "       0.28867513, 0.28867513]), 'feedback': None}\n",
      "Reward: 0\n",
      "Done: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated response length: 20\n",
      "Observation: {'user_question': array([0.5, 0.5, 0.5, 0.5]), 'chatbot_response': array([0.57735027, 0.57735027, 0.57735027]), 'feedback': None}\n",
      "Reward: 0\n",
      "Done: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated response is empty or repetitive. Resetting conversation history.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated response length: 71\n",
      "Observation: {'user_question': array([0.5, 0.5, 0.5, 0.5]), 'chatbot_response': array([0.2773501, 0.2773501, 0.2773501, 0.2773501, 0.2773501, 0.2773501,\n",
      "       0.2773501, 0.2773501, 0.5547002, 0.2773501]), 'feedback': None}\n",
      "Reward: 0\n",
      "Done: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated response length: 44\n",
      "Observation: {'user_question': array([0.5, 0.5, 0.5, 0.5]), 'chatbot_response': array([0.35355339, 0.35355339, 0.35355339, 0.35355339, 0.35355339,\n",
      "       0.35355339, 0.35355339, 0.35355339]), 'feedback': None}\n",
      "Reward: 0\n",
      "Done: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated response length: 76\n",
      "Observation: {'user_question': array([0.5, 0.5, 0.5, 0.5]), 'chatbot_response': array([0.30151134, 0.30151134, 0.30151134, 0.30151134, 0.30151134,\n",
      "       0.30151134, 0.30151134, 0.30151134, 0.30151134, 0.30151134,\n",
      "       0.30151134]), 'feedback': None}\n",
      "Reward: 0\n",
      "Done: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated response length: 87\n",
      "Observation: {'user_question': array([0.5, 0.5, 0.5, 0.5]), 'chatbot_response': array([0.24253563, 0.48507125, 0.24253563, 0.24253563, 0.24253563,\n",
      "       0.24253563, 0.48507125, 0.24253563, 0.24253563, 0.24253563,\n",
      "       0.24253563]), 'feedback': None}\n",
      "Reward: 0\n",
      "Done: True\n"
     ]
    }
   ],
   "source": [
    "CHATBOT_API_URL = \"http://127.0.0.1:5000/chat\"\n",
    "MAX_EPISODE_LENGTH = 10\n",
    "CONVERSATION_JSON = CHAT_HISTORY_PATH\n",
    "CHATBOT_MODEL = chatbot\n",
    "\n",
    "# Create the ChatbotEnv environment\n",
    "env = ChatbotEnv(CHATBOT_API_URL, MAX_EPISODE_LENGTH, CONVERSATION_JSON, CHATBOT_MODEL)\n",
    "\n",
    "# Reset the environment to start a new episode\n",
    "observation = env.reset()\n",
    "\n",
    "# Run steps\n",
    "for _ in range(MAX_EPISODE_LENGTH):\n",
    "    # Action for chatbot (in this case, there's only one action: 0)\n",
    "    action = 0\n",
    "\n",
    "    # Take a step in the environment based on the action\n",
    "    observation, reward, done, info = env.step(action)\n",
    "\n",
    "    # Print the current observation, reward, and whether the episode is done\n",
    "    print(\"Observation:\", observation)\n",
    "    print(\"Reward:\", reward)\n",
    "    print(\"Done:\", done)\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "# Close the environment when done\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatterbot_env_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
